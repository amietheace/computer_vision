{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"amazon_from_space_Transfer-learning(VGG-16)_with_fine-tuned_weights.ipynb","version":"0.3.2","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4iGg9YMTYCa9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"f0ebc18f-aff8-49cf-eaef-1579c103fca4","executionInfo":{"status":"ok","timestamp":1563996539508,"user_tz":-330,"elapsed":851,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["import pandas as pd\n","\n","mapping_csv = pd.read_csv('/content/data/train_v2.csv')\n","# summarize properties\n","print(mapping_csv.shape)\n","print(mapping_csv[:10])"],"execution_count":1,"outputs":[{"output_type":"stream","text":["(40479, 2)\n","  image_name                                         tags\n","0    train_0                                 haze primary\n","1    train_1              agriculture clear primary water\n","2    train_2                                clear primary\n","3    train_3                                clear primary\n","4    train_4    agriculture clear habitation primary road\n","5    train_5                           haze primary water\n","6    train_6  agriculture clear cultivation primary water\n","7    train_7                                 haze primary\n","8    train_8        agriculture clear cultivation primary\n","9    train_9   agriculture clear cultivation primary road\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VC6Ufx9QYMrq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"120431ac-d55e-4066-b04f-a3a8ad2b4ce6","executionInfo":{"status":"ok","timestamp":1563996563608,"user_tz":-330,"elapsed":1430,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# create a set of labels\n","labels = set()\n","for i in range(len(mapping_csv)):\n","  # convert spaced separated tags into an array of tags\n","  tags = mapping_csv['tags'][i].split(' ')\n","  # add tags to the set of known labels\n","  labels.update(tags)\n","  \n","\n","\n","\n","# create a mapping of tags to integers given the loaded mapping file\n","def create_tag_mapping(mapping_csv):\n","\t# create a set of all known tags\n","\tlabels = set()\n","\tfor i in range(len(mapping_csv)):\n","\t\t# convert spaced separated tags into an array of tags\n","\t\ttags = mapping_csv['tags'][i].split(' ')\n","\t\t# add tags to the set of known labels\n","\t\tlabels.update(tags)\n","\t# convert set of labels to a list to list\n","\tlabels = list(labels)\n","\t# order set alphabetically\n","\tlabels.sort()\n","\t# dict that maps labels to integers, and the reverse\n","\tlabels_map = {labels[i]:i for i in range(len(labels))}\n","\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n","\treturn labels_map, inv_labels_map\n","\n","mapping, inv_mapping = create_tag_mapping(mapping_csv)\n","print(len(mapping))\n","print(mapping)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["17\n","{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lYKEqXXoYXLd","colab_type":"code","colab":{}},"source":["# create a mapping of filename to tags\n","def create_file_mapping(mapping_csv):\n","  mapping = dict()\n","  for i in range(len(mapping_csv)):\n","    name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n","    mapping[name] = tags.split(' ')\n","  return mapping\n","\n","# create a one hot encoding for one list of tags\n","def one_hot_encode(tags, mapping):\n","\t# create empty vector\n","\tencoding = zeros(len(mapping), dtype='uint8')\n","\t# mark 1 for each tag in the vector\n","\tfor tag in tags:\n","\t\tencoding[mapping[tag]] = 1\n","\treturn encoding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1IAqIFcYem2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"17249d35-641f-407f-b973-b43c3d580be6","executionInfo":{"status":"ok","timestamp":1563996774619,"user_tz":-330,"elapsed":207450,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# load and prepare planet dataset and save to file\n","from os import listdir\n","from numpy import zeros\n","from numpy import asarray\n","from numpy import savez_compressed\n","from pandas import read_csv\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","\n","# load all images into memory\n","def load_dataset(path, file_mapping, tag_mapping):\n","  photos, targets = list(), list()\n","  # enumerate files in the directory\n","  for filename in listdir(folder):\n","    # load image\n","    photo = load_img(path + filename, target_size=(128,128))\n","    # convert to numpy array\n","    photo = img_to_array(photo, dtype='uint8')\n","    # get tags\n","    tags = file_mapping[filename[:-4]]\n","    # one hot encode tags\n","    target = one_hot_encode(tags, tag_mapping)\n","    # store\n","    photos.append(photo)\n","    targets.append(target)\n","  X = asarray(photos, dtype='uint8')\n","  y = asarray(targets, dtype='uint8')\n","  return X, y\n","\n","# load the mapping file\n","filename = '/content/data/train_v2.csv'\n","mapping_csv = read_csv(filename)\n","# create a mapping of tags to integers\n","tag_mapping, _ = create_tag_mapping(mapping_csv)\n","# create a mapping of filenames to tag lists\n","file_mapping = create_file_mapping(mapping_csv)\n","# load the jpeg images\n","folder = '/content/data/train-jpg/'\n","\n","X, y = load_dataset(folder, file_mapping, tag_mapping)\n","print(X.shape, y.shape)\n","\n","# save both arrays to one file in compressed format\n","savez_compressed('planet_data.npz', X, y)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["(40479, 128, 128, 3) (40479, 17)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lCEeopR-Z9N4","colab_type":"text"},"source":["The VGG-16 model was designed to classify photographs of objects into one of 1,000 categories. As such, it was designed to pick out fine-grained features of objects. We can guess that the features learned by the model by the deeper layers will represent higher order features seen in the ImageNet dataset that may not be directly relevant to the classification of satellite photos of the Amazon rainforest.\n","\n","To address this, we can re-fit the VGG-16 model and allow the training algorithm to fine tune the weights for some of the layers in the model. In this case, we will make the three convolutional layers (and pooling layer for consistency) as trainable."]},{"cell_type":"code","metadata":{"id":"t_R68eeFanT9","colab_type":"code","colab":{}},"source":["# vgg16 transfer learning on the planet dataset\n","import sys\n","from numpy import load\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from keras import backend\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0U_qtK8luZ7","colab_type":"code","colab":{}},"source":["# load train and test dataset\n","def load_dataset():\n","  # load dataset\n","  data = load('planet_data.npz')\n","  X, y = data['arr_0'], data['arr_1']\n","  # separate into train and test datasets\n","  trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n","  print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n","  return trainX, trainY, testX, testY"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuPpyDnal12p","colab_type":"code","colab":{}},"source":["# calculate fbeta score for multi-class/label classification\n","def fbeta(y_true, y_pred, beta=2):\n","  # clip predictions\n","  y_pred = backend.clip(y_pred, 0, 1)\n","  # calculate elements\n","  tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n","  fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n","  fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n","  # calculate precision\n","  p = tp / (tp + fp + backend.epsilon())\n","  # calculate recall\n","  r = tp / (tp + fn + backend.epsilon())\n","  # calculate fbeta, averaged across each class\n","  bb = beta ** 2\n","  fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n","  return fbeta_score\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tupfm_JPl9YL","colab_type":"code","colab":{}},"source":["# define cnn model with fine tuning\n","def define_model(in_shape=(128, 128, 3), out_shape=17):\n","  # load model\n","  model = VGG16(include_top=False, input_shape=in_shape)\n","  # mark loaded layers as not trainable\n","  for layer in model.layers:\n","    layer.trainable = False\n","  # allow last vgg block to be trainable\n","  model.get_layer('block5_conv1').trainable = True\n","  model.get_layer('block5_conv2').trainable = True\n","  model.get_layer('block5_conv3').trainable = True\n","  model.get_layer('block5_pool').trainable = True\n","  # add new classifier layers\n","  flat1 = Flatten()(model.layers[-1].output)\n","  class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n","  output = Dense(out_shape, activation='sigmoid')(class1)\n","  # define new model\n","  model = Model(inputs=model.inputs, outputs=output)\n","  # compile model\n","  opt = SGD(lr=0.01, momentum=0.9)\n","  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChXqOFvWmA52","colab_type":"code","colab":{}},"source":["# plot diagnostic learning curves\n","def summarize_diagnostics(history):\n","\t# plot loss\n","\tpyplot.subplot(211)\n","\tpyplot.title('Cross Entropy Loss')\n","\tpyplot.plot(history.history['loss'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n","\t# plot accuracy\n","\tpyplot.subplot(212)\n","\tpyplot.title('Fbeta')\n","\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n","\t# save plot to file\n","\tfilename = sys.argv[0].split('/')[-1]\n","\tpyplot.savefig(filename + '_plot.png')\n","\tpyplot.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh7LTms1oVup","colab_type":"code","colab":{}},"source":["# run the test harness for evaluating a model\n","def run_test_harness():\n","\t# load dataset\n","\ttrainX, trainY, testX, testY = load_dataset()\n","\t# create data generator\n","\tdatagen = ImageDataGenerator(featurewise_center=True)\n","\t# specify imagenet mean values for centering\n","\tdatagen.mean = [123.68, 116.779, 103.939]\n","\t# prepare iterators\n","\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n","\ttest_it = datagen.flow(testX, testY, batch_size=128)\n","\t# define model\n","\tmodel = define_model()\n","\t# fit model\n","\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n","\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n","\t# evaluate model\n","\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n","\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n","\t# learning curves\n","\tsummarize_diagnostics(history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToYtTka_odxM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"fa00170c-91d6-4832-8fd5-6760bc6826d8","executionInfo":{"status":"ok","timestamp":1564000917841,"user_tz":-330,"elapsed":532485,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# entry point, run the test harness\n","run_test_harness()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17)\n","> loss=0.233, fbeta=0.879\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v7z5zwigvVD9","colab_type":"text"},"source":["![VGG-16_Fine_tuned](https://drive.google.com/uc?id=1omSWes9onut50F37SWfijGekBLoTdrCJ)"]},{"cell_type":"code","metadata":{"id":"WaquFel_omcU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}