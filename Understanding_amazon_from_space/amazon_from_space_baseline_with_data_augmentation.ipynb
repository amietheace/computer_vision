{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_from_space_baseline_with_data_augmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyQox4C5Ksff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modify according to kaggle username and key \n",
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"amitkagglepractice\",\"key\":\"fbb6aeed8826cde8a23312712a265de1\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ImPXpVKyho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_qEfaPFK6cP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "77ff41a5-9a5b-4bc9-8914-daa36f7ff529"
      },
      "source": [
        "# train_v2.csv \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p data\n",
        "# unzipping\n",
        "% cd /content/data\n",
        "!unzip train_v2.csv.zip -d /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading train_v2.csv.zip to data\n",
            "  0% 0.00/159k [00:00<?, ?B/s]\n",
            "100% 159k/159k [00:00<00:00, 23.9MB/s]\n",
            "/content/data\n",
            "Archive:  train_v2.csv.zip\n",
            "  inflating: /content/data/train_v2.csv  \n",
            "   creating: /content/data/__MACOSX/\n",
            "  inflating: /content/data/__MACOSX/._train_v2.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB_I9uulK-Rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "5cfd26d7-6123-4b5b-d920-e691722d2c3c"
      },
      "source": [
        "# train-jpg.tar.7z\n",
        "# this should take a little time - loading all the images \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p data\n",
        "# unzipping \n",
        "% cd /content/data\n",
        "!7z x -so train-jpg.tar.7z | tar xf - -C /content/data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading train-jpg.tar.7z to data\n",
            "100% 600M/600M [00:12<00:00, 71.7MB/s]\n",
            "100% 600M/600M [00:12<00:00, 51.7MB/s]\n",
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O19BKxozLDrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d9a9c85b-ab8f-43ee-c5fa-10e667f3bb3d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "mapping_csv = pd.read_csv('/content/data/train_v2.csv')\n",
        "# summarize properties\n",
        "print(mapping_csv.shape)\n",
        "print(mapping_csv[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40479, 2)\n",
            "  image_name                                         tags\n",
            "0    train_0                                 haze primary\n",
            "1    train_1              agriculture clear primary water\n",
            "2    train_2                                clear primary\n",
            "3    train_3                                clear primary\n",
            "4    train_4    agriculture clear habitation primary road\n",
            "5    train_5                           haze primary water\n",
            "6    train_6  agriculture clear cultivation primary water\n",
            "7    train_7                                 haze primary\n",
            "8    train_8        agriculture clear cultivation primary\n",
            "9    train_9   agriculture clear cultivation primary road\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYkvfZDELngI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "43aba6d4-8f49-4b29-d458-15670f73775a"
      },
      "source": [
        "# create a set of labels\n",
        "labels = set()\n",
        "for i in range(len(mapping_csv)):\n",
        "  # convert spaced separated tags into an array of tags\n",
        "  tags = mapping_csv['tags'][i].split(' ')\n",
        "  # add tags to the set of known labels\n",
        "  labels.update(tags)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# create a mapping of tags to integers given the loaded mapping file\n",
        "def create_tag_mapping(mapping_csv):\n",
        "\t# create a set of all known tags\n",
        "\tlabels = set()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\t# convert spaced separated tags into an array of tags\n",
        "\t\ttags = mapping_csv['tags'][i].split(' ')\n",
        "\t\t# add tags to the set of known labels\n",
        "\t\tlabels.update(tags)\n",
        "\t# convert set of labels to a list to list\n",
        "\tlabels = list(labels)\n",
        "\t# order set alphabetically\n",
        "\tlabels.sort()\n",
        "\t# dict that maps labels to integers, and the reverse\n",
        "\tlabels_map = {labels[i]:i for i in range(len(labels))}\n",
        "\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "\treturn labels_map, inv_labels_map\n",
        "\n",
        "mapping, inv_mapping = create_tag_mapping(mapping_csv)\n",
        "print(len(mapping))\n",
        "print(mapping)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SxVymeHMOXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping of filename to tags\n",
        "def create_file_mapping(mapping_csv):\n",
        "  mapping = dict()\n",
        "  for i in range(len(mapping_csv)):\n",
        "    name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
        "    mapping[name] = tags.split(' ')\n",
        "  return mapping\n",
        "\n",
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "\t# create empty vector\n",
        "\tencoding = zeros(len(mapping), dtype='uint8')\n",
        "\t# mark 1 for each tag in the vector\n",
        "\tfor tag in tags:\n",
        "\t\tencoding[mapping[tag]] = 1\n",
        "\treturn encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0N8WbqCMdxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9fc4e190-d02c-4281-beeb-7801843bebff"
      },
      "source": [
        "# load and prepare planet dataset and save to file\n",
        "from os import listdir\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from pandas import read_csv\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# load all images into memory\n",
        "def load_dataset(path, file_mapping, tag_mapping):\n",
        "\tphotos, targets = list(), list()\n",
        "\t# enumerate files in the directory\n",
        "\tfor filename in listdir(folder):\n",
        "\t\t# load image\n",
        "\t\tphoto = load_img(path + filename, target_size=(128,128))\n",
        "\t\t# convert to numpy array\n",
        "\t\tphoto = img_to_array(photo, dtype='uint8')\n",
        "\t\t# get tags\n",
        "\t\ttags = file_mapping[filename[:-4]]\n",
        "\t\t# one hot encode tags\n",
        "\t\ttarget = one_hot_encode(tags, tag_mapping)\n",
        "\t\t# store\n",
        "\t\tphotos.append(photo)\n",
        "\t\ttargets.append(target)\n",
        "\tX = asarray(photos, dtype='uint8')\n",
        "\ty = asarray(targets, dtype='uint8')\n",
        "\treturn X, y\n",
        "\n",
        "# load the mapping file\n",
        "filename = '/content/data/train_v2.csv'\n",
        "mapping_csv = read_csv(filename)\n",
        "# create a mapping of tags to integers\n",
        "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
        "# create a mapping of filenames to tag lists\n",
        "file_mapping = create_file_mapping(mapping_csv)\n",
        "# load the jpeg images\n",
        "folder = '/content/data/train-jpg/'\n",
        "\n",
        "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# save both arrays to one file in compressed format\n",
        "savez_compressed('planet_data.npz', X, y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(40479, 128, 128, 3) (40479, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXulJqi2OYR2",
        "colab_type": "text"
      },
      "source": [
        "## Image Data Augmentation\n",
        "\n",
        "Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n",
        "\n",
        "Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n",
        "\n",
        "Data augmentation can also act as a regularization technique, adding noise to the training data and encouraging the model to learn the same features, invariant to their position in the input.\n",
        "\n",
        "Small changes to the input photos of the satellite photos might be useful for this problem, such as horizontal flips, vertical flips, rotations, zooms, and perhaps more. These augmentations can be specified as arguments to the ImageDataGenerator instance, used for the training dataset. The augmentations should not be used for the test dataset, as we wish to evaluate the performance of the model on the unmodified photographs.\n",
        "\n",
        "This requires that we have a separate ImageDataGenerator instance for the train and test dataset, then iterators for the train and test sets created from the respective data generators. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7qIvNNAM0pU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "dcce6630-706b-4f91-f021-613bf567a086"
      },
      "source": [
        "# baseline model with data augmentation for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        " \n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        " \n",
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(out_shape, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\ttest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0)\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        " \n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 18:34:37.060088 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 18:34:37.097626 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 18:34:37.107170 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 18:34:37.172387 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0724 18:34:37.297473 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0724 18:34:37.306339 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0724 18:34:37.313484 139687471814528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0724 18:34:37.793476 139687471814528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnIAwtyYPDMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}