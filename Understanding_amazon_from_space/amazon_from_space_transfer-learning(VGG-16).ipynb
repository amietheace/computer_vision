{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"amazon_from_space_transfer-learning(VGG-16).ipynb","version":"0.3.2","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"thbDoPncQ5sd","colab_type":"code","colab":{}},"source":["# modify according to kaggle username and key \n","\n","!pip install -U -q kaggle\n","!mkdir -p ~/.kaggle\n","!echo '{\"username\":\"amitkagglepractice\",\"key\":\"fbb6aeed8826cde8a23312712a265de1\"}' > ~/.kaggle/kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMa2KJINREGu","colab_type":"code","colab":{}},"source":["!mkdir -p data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYZwbu3oRSde","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"outputId":"00d5b793-4f40-43a8-f002-786745568ab9","executionInfo":{"status":"ok","timestamp":1563994242965,"user_tz":-330,"elapsed":70305,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# train_v2.csv \n","\n","# downloading from kaggle \n","% cd /content\n","!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p data\n","# unzipping\n","% cd /content/data\n","!unzip train_v2.csv.zip -d /content/data\n","\n","# train-jpg.tar.7z\n","# this should take a little time - loading all the images \n","\n","# downloading from kaggle \n","% cd /content\n","!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p data\n","# unzipping \n","% cd /content/data\n","!7z x -so train-jpg.tar.7z | tar xf - -C /content/data"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content\n","train_v2.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n","/content/data\n","Archive:  train_v2.csv.zip\n","replace /content/data/train_v2.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace /content/data/__MACOSX/._train_v2.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","/content\n","train-jpg.tar.7z: Skipping, found more recently modified local copy (use --force to force download)\n","/content/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yji2GjWxRbP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"0783d9aa-a909-40df-becf-6564ae8812b2","executionInfo":{"status":"ok","timestamp":1563994288882,"user_tz":-330,"elapsed":947,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["\n","import pandas as pd\n","\n","mapping_csv = pd.read_csv('/content/data/train_v2.csv')\n","# summarize properties\n","print(mapping_csv.shape)\n","print(mapping_csv[:10])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(40479, 2)\n","  image_name                                         tags\n","0    train_0                                 haze primary\n","1    train_1              agriculture clear primary water\n","2    train_2                                clear primary\n","3    train_3                                clear primary\n","4    train_4    agriculture clear habitation primary road\n","5    train_5                           haze primary water\n","6    train_6  agriculture clear cultivation primary water\n","7    train_7                                 haze primary\n","8    train_8        agriculture clear cultivation primary\n","9    train_9   agriculture clear cultivation primary road\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B1A1BxzKSAnI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"f9d934f1-274f-49dd-8f87-b863c8d0fb17","executionInfo":{"status":"ok","timestamp":1563994294102,"user_tz":-330,"elapsed":2226,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# create a set of labels\n","labels = set()\n","for i in range(len(mapping_csv)):\n","  # convert spaced separated tags into an array of tags\n","  tags = mapping_csv['tags'][i].split(' ')\n","  # add tags to the set of known labels\n","  labels.update(tags)\n","  \n","\n","\n","\n","# create a mapping of tags to integers given the loaded mapping file\n","def create_tag_mapping(mapping_csv):\n","\t# create a set of all known tags\n","\tlabels = set()\n","\tfor i in range(len(mapping_csv)):\n","\t\t# convert spaced separated tags into an array of tags\n","\t\ttags = mapping_csv['tags'][i].split(' ')\n","\t\t# add tags to the set of known labels\n","\t\tlabels.update(tags)\n","\t# convert set of labels to a list to list\n","\tlabels = list(labels)\n","\t# order set alphabetically\n","\tlabels.sort()\n","\t# dict that maps labels to integers, and the reverse\n","\tlabels_map = {labels[i]:i for i in range(len(labels))}\n","\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n","\treturn labels_map, inv_labels_map\n","\n","mapping, inv_mapping = create_tag_mapping(mapping_csv)\n","print(len(mapping))\n","print(mapping)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["17\n","{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zbeTlZdBSGnh","colab_type":"code","colab":{}},"source":["# create a mapping of filename to tags\n","def create_file_mapping(mapping_csv):\n","  mapping = dict()\n","  for i in range(len(mapping_csv)):\n","    name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n","    mapping[name] = tags.split(' ')\n","  return mapping\n","\n","# create a one hot encoding for one list of tags\n","def one_hot_encode(tags, mapping):\n","\t# create empty vector\n","\tencoding = zeros(len(mapping), dtype='uint8')\n","\t# mark 1 for each tag in the vector\n","\tfor tag in tags:\n","\t\tencoding[mapping[tag]] = 1\n","\treturn encoding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMEM2WhiSg2i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0d71a97f-3bbc-46a4-8a3d-0d50663e114b","executionInfo":{"status":"ok","timestamp":1563994487389,"user_tz":-330,"elapsed":187719,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# load and prepare planet dataset and save to file\n","from os import listdir\n","from numpy import zeros\n","from numpy import asarray\n","from numpy import savez_compressed\n","from pandas import read_csv\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","\n","# load all images into memory\n","def load_dataset(path, file_mapping, tag_mapping):\n","  photos, targets = list(), list()\n","  # enumerate files in the directory\n","  for filename in listdir(folder):\n","    # load image\n","    photo = load_img(path + filename, target_size=(128,128))\n","    # convert to numpy array\n","    photo = img_to_array(photo, dtype='uint8')\n","    # get tags\n","    tags = file_mapping[filename[:-4]]\n","    # one hot encode tags\n","    target = one_hot_encode(tags, tag_mapping)\n","    # store\n","    photos.append(photo)\n","    targets.append(target)\n","  X = asarray(photos, dtype='uint8')\n","  y = asarray(targets, dtype='uint8')\n","  return X, y\n","\n","# load the mapping file\n","filename = '/content/data/train_v2.csv'\n","mapping_csv = read_csv(filename)\n","# create a mapping of tags to integers\n","tag_mapping, _ = create_tag_mapping(mapping_csv)\n","# create a mapping of filenames to tag lists\n","file_mapping = create_file_mapping(mapping_csv)\n","# load the jpeg images\n","folder = '/content/data/train-jpg/'\n","\n","X, y = load_dataset(folder, file_mapping, tag_mapping)\n","print(X.shape, y.shape)\n","\n","# save both arrays to one file in compressed format\n","savez_compressed('planet_data.npz', X, y)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(40479, 128, 128, 3) (40479, 17)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"71-B1RVOTDup","colab_type":"text"},"source":["## Use Transfer Learning\n","\n","Transfer learning involves using all or parts of a model trained on a related task.\n","\n","Keras provides a range of pre-trained models that can be loaded and used wholly or partially via the Keras Applications API.\n","\n","A useful model for transfer learning is one of the VGG models, such as VGG-16 with 16 layers that, at the time it was developed, achieved top results on the ImageNet photo classification challenge.\n","\n","The model is comprised of two main parts: the feature extractor part of the model that is made up of VGG blocks, and the classifier part of the model that is made up of fully connected layers and the output layer.\n","\n","We can use the feature extraction part of the model and add a new classifier part of the model that is tailored to the planets dataset. Specifically, we can hold the weights of all of the convolutional layers fixed during training and only train new fully connected layers that will learn to interpret the features extracted from the model and make a suite of binary classifications.\n","\n","This can be achieved by loading the VGG-16 model, removing the fully connected layers from the output-end of the model, then adding the new fully connected layers to interpret the model output and make a prediction. The classifier part of the model can be removed automatically by setting the “include_top” argument to “False“, which also requires that the shape of the input be specified for the model, in this case (128, 128, 3). This means that the loaded model ends at the last max pooling layer, after which we can manually add a Flatten layer and the new classifier fully-connected layers.\n","\n","\n","Not a lot of training will be required in this case, as only the new fully connected and output layers have trainable weights. As such, we will fix the number of training epochs at 10.\n","\n","The VGG16 model was trained on a specific ImageNet challenge dataset. As such, the model expects images to be centered. That is, to have the mean pixel values from each channel (red, green, and blue) as calculated on the ImageNet training dataset subtracted from the input.\n","\n","Keras provides a function to perform this preparation for individual photos via the preprocess_input() function. Nevertheless, we can achieve the same effect with the image data generator, by setting the “featurewise_center” argument to “True” and manually specifying the mean pixel values to use when centering as the mean values from the ImageNet training dataset: [123.68, 116.779, 103.939]."]},{"cell_type":"code","metadata":{"id":"yS48LOMwSm7H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":437},"outputId":"f51e08f4-44a5-42f4-9d10-00259cda4186","executionInfo":{"status":"ok","timestamp":1563995837718,"user_tz":-330,"elapsed":1334158,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# vgg16 transfer learning on the planet dataset\n","import sys\n","from numpy import load\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from keras import backend\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# load train and test dataset\n","def load_dataset():\n","\t# load dataset\n","\tdata = load('planet_data.npz')\n","\tX, y = data['arr_0'], data['arr_1']\n","\t# separate into train and test datasets\n","\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n","\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n","\treturn trainX, trainY, testX, testY\n","\n","# calculate fbeta score for multi-class/label classification\n","def fbeta(y_true, y_pred, beta=2):\n","\t# clip predictions\n","\ty_pred = backend.clip(y_pred, 0, 1)\n","\t# calculate elements\n","\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n","\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n","\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n","\t# calculate precision\n","\tp = tp / (tp + fp + backend.epsilon())\n","\t# calculate recall\n","\tr = tp / (tp + fn + backend.epsilon())\n","\t# calculate fbeta, averaged across each class\n","\tbb = beta ** 2\n","\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n","\treturn fbeta_score\n","\n","# define cnn model\n","def define_model(in_shape=(128, 128, 3), out_shape=17):\n","\t# load model\n","\tmodel = VGG16(include_top=False, input_shape=in_shape)\n","\t# mark loaded layers as not trainable\n","\tfor layer in model.layers:\n","\t\tlayer.trainable = False\n","\t# add new classifier layers\n","\tflat1 = Flatten()(model.layers[-1].output)\n","\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n","\toutput = Dense(out_shape, activation='sigmoid')(class1)\n","\t# define new model\n","\tmodel = Model(inputs=model.inputs, outputs=output)\n","\t# compile model\n","\topt = SGD(lr=0.01, momentum=0.9)\n","\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n","\treturn model\n","\n","# plot diagnostic learning curves\n","def summarize_diagnostics(history):\n","\t# plot loss\n","\tpyplot.subplot(211)\n","\tpyplot.title('Cross Entropy Loss')\n","\tpyplot.plot(history.history['loss'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n","\t# plot accuracy\n","\tpyplot.subplot(212)\n","\tpyplot.title('Fbeta')\n","\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n","\t# save plot to file\n","\tfilename = sys.argv[0].split('/')[-1]\n","\tpyplot.savefig(filename + '_plot.png')\n","\tpyplot.close()\n","\n","# run the test harness for evaluating a model\n","def run_test_harness():\n","\t# load dataset\n","\ttrainX, trainY, testX, testY = load_dataset()\n","\t# create data generator\n","\tdatagen = ImageDataGenerator(featurewise_center=True)\n","\t# specify imagenet mean values for centering\n","\tdatagen.mean = [123.68, 116.779, 103.939]\n","\t# prepare iterators\n","\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n","\ttest_it = datagen.flow(testX, testY, batch_size=128)\n","\t# define model\n","\tmodel = define_model()\n","\t# fit model\n","\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n","\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n","\t# evaluate model\n","\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n","\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n","\t# learning curves\n","\tsummarize_diagnostics(history)\n","\n","# entry point, run the test harness\n","run_test_harness()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17)\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0724 18:55:26.444516 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0724 18:55:26.867389 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0724 18:55:26.941908 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0724 18:55:27.016349 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["W0724 18:55:28.569824 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0724 18:55:28.570750 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0724 18:55:32.882739 139667291637632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0724 18:55:32.894355 139667291637632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["> loss=0.151, fbeta=0.859\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yZbMwggBd-tp","colab_type":"text"},"source":["![learning_curve](https://drive.google.com/uc?id=1uzKE-DKv0et3SPwelmrxfbBeisg6hH-P)"]},{"cell_type":"code","metadata":{"id":"zeH46M1dbMUg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}