{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_from_space_VGG-16_with_fine_tuning_and_data_augmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPsUnpfKwGld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modify according to kaggle username and key \n",
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"amitkagglepractice\",\"key\":\"fbb6aeed8826cde8a23312712a265de1\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byKZ0LpDwrbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St0mrm16w0hx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "ec36fc51-76d9-4c41-a4c9-1a61c72d302c"
      },
      "source": [
        "# train_v2.csv \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p data\n",
        "# unzipping\n",
        "% cd /content/data\n",
        "!unzip train_v2.csv.zip -d /content/data\n",
        "\n",
        "# train-jpg.tar.7z\n",
        "# this should take a little time - loading all the images \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p data\n",
        "# unzipping \n",
        "% cd /content/data\n",
        "!7z x -so train-jpg.tar.7z | tar xf - -C /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "train_v2.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "/content/data\n",
            "Archive:  train_v2.csv.zip\n",
            "replace /content/data/train_v2.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/data/__MACOSX/._train_v2.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "/content\n",
            "train-jpg.tar.7z: Skipping, found more recently modified local copy (use --force to force download)\n",
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu43xqb1w9_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1ebd286b-c42c-4d92-b90b-9acc4c4edff1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "mapping_csv = pd.read_csv('/content/data/train_v2.csv')\n",
        "# summarize properties\n",
        "print(mapping_csv.shape)\n",
        "print(mapping_csv[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40479, 2)\n",
            "  image_name                                         tags\n",
            "0    train_0                                 haze primary\n",
            "1    train_1              agriculture clear primary water\n",
            "2    train_2                                clear primary\n",
            "3    train_3                                clear primary\n",
            "4    train_4    agriculture clear habitation primary road\n",
            "5    train_5                           haze primary water\n",
            "6    train_6  agriculture clear cultivation primary water\n",
            "7    train_7                                 haze primary\n",
            "8    train_8        agriculture clear cultivation primary\n",
            "9    train_9   agriculture clear cultivation primary road\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK4WN8tqxav5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "67b2b524-078f-433d-9a89-d58e40b717bf"
      },
      "source": [
        "# create a set of labels\n",
        "labels = set()\n",
        "for i in range(len(mapping_csv)):\n",
        "  # convert spaced separated tags into an array of tags\n",
        "  tags = mapping_csv['tags'][i].split(' ')\n",
        "  # add tags to the set of known labels\n",
        "  labels.update(tags)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# create a mapping of tags to integers given the loaded mapping file\n",
        "def create_tag_mapping(mapping_csv):\n",
        "\t# create a set of all known tags\n",
        "\tlabels = set()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\t# convert spaced separated tags into an array of tags\n",
        "\t\ttags = mapping_csv['tags'][i].split(' ')\n",
        "\t\t# add tags to the set of known labels\n",
        "\t\tlabels.update(tags)\n",
        "\t# convert set of labels to a list to list\n",
        "\tlabels = list(labels)\n",
        "\t# order set alphabetically\n",
        "\tlabels.sort()\n",
        "\t# dict that maps labels to integers, and the reverse\n",
        "\tlabels_map = {labels[i]:i for i in range(len(labels))}\n",
        "\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "\treturn labels_map, inv_labels_map\n",
        "\n",
        "mapping, inv_mapping = create_tag_mapping(mapping_csv)\n",
        "print(len(mapping))\n",
        "print(mapping)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxXD1AEwxgi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping of filename to tags\n",
        "def create_file_mapping(mapping_csv):\n",
        "  mapping = dict()\n",
        "  for i in range(len(mapping_csv)):\n",
        "    name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
        "    mapping[name] = tags.split(' ')\n",
        "  return mapping\n",
        "\n",
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "\t# create empty vector\n",
        "\tencoding = zeros(len(mapping), dtype='uint8')\n",
        "\t# mark 1 for each tag in the vector\n",
        "\tfor tag in tags:\n",
        "\t\tencoding[mapping[tag]] = 1\n",
        "\treturn encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdcfra__xqfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a34360a8-3014-4995-cff4-df3a17f45dca"
      },
      "source": [
        "# load and prepare planet dataset and save to file\n",
        "from os import listdir\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from pandas import read_csv\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# load all images into memory\n",
        "def load_dataset(path, file_mapping, tag_mapping):\n",
        "  photos, targets = list(), list()\n",
        "  # enumerate files in the directory\n",
        "  for filename in listdir(folder):\n",
        "    # load image\n",
        "    photo = load_img(path + filename, target_size=(128,128))\n",
        "    # convert to numpy array\n",
        "    photo = img_to_array(photo, dtype='uint8')\n",
        "    # get tags\n",
        "    tags = file_mapping[filename[:-4]]\n",
        "    # one hot encode tags\n",
        "    target = one_hot_encode(tags, tag_mapping)\n",
        "    # store\n",
        "    photos.append(photo)\n",
        "    targets.append(target)\n",
        "  X = asarray(photos, dtype='uint8')\n",
        "  y = asarray(targets, dtype='uint8')\n",
        "  return X, y\n",
        "\n",
        "# load the mapping file\n",
        "filename = '/content/data/train_v2.csv'\n",
        "mapping_csv = read_csv(filename)\n",
        "# create a mapping of tags to integers\n",
        "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
        "# create a mapping of filenames to tag lists\n",
        "file_mapping = create_file_mapping(mapping_csv)\n",
        "# load the jpeg images\n",
        "folder = '/content/data/train-jpg/'\n",
        "\n",
        "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# save both arrays to one file in compressed format\n",
        "savez_compressed('planet_data.npz', X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9LT9s1CyVox",
        "colab_type": "text"
      },
      "source": [
        "Given that we saw a large improvement with the use of data augmentation on the baseline model, it may be interesting to see if data augmentation can be used to improve the performance of the VGG-16 model with fine-tuning.\n",
        "\n",
        "In this case, the same define_model() function can be used, although in this case the run_test_harness() can be updated to use image data augmentation as was performed in the previous section. We expect that the addition of data augmentation will slow the rate of improvement. As such we will increase the number of training epochs from 20 to 50 to give the model more time to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0SKLUJoy9uX",
        "colab_type": "text"
      },
      "source": [
        "#### Importing required libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyz6S1_xvI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d19329d4-8b61-4215-f343-25fa913e75b8"
      },
      "source": [
        "# vgg with fine-tuning and data augmentation for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vn4c0V6zGZT",
        "colab_type": "text"
      },
      "source": [
        "#### Loading dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znqq-8vNyk7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvmBEc5GzOOf",
        "colab_type": "text"
      },
      "source": [
        "#### Defining Evaluation metrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsYcnnrCypOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TkThCR9zXej",
        "colab_type": "text"
      },
      "source": [
        "#### Defining model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4j9JFL1yuN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# allow last vgg block to be trainable\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIVYe_uvzgvY",
        "colab_type": "text"
      },
      "source": [
        "#### Defining function for plotting Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqPWPrdkyy5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht9YSQUhzrkc",
        "colab_type": "text"
      },
      "source": [
        "#### Defining function to execute training and all above steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLu6FwEGy3V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n",
        "\t# specify imagenet mean values for centering\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\ttest_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLsgAshwz1a9",
        "colab_type": "text"
      },
      "source": [
        "#### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ua4PN4My6hC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "5c0b9c3f-05ba-498e-beb7-f3833ec70080"
      },
      "source": [
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 21:42:30.056152 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 21:42:30.068248 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 21:42:30.071509 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 21:42:30.098088 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0724 21:42:30.638939 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0724 21:42:30.640617 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0724 21:42:31.876217 140426489243520 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0724 21:42:31.886579 140426489243520 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "> loss=0.100, fbeta=0.897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeqbOb-4T-5b",
        "colab_type": "text"
      },
      "source": [
        "![VGG-16+FT+DA](https://drive.google.com/uc?id=15461OKFZQz7Uok7lIdvNjcF6PNX8APXI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGxGZQ5cz5Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}