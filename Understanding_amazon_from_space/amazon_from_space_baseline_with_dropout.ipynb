{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_from_space_baseline_with_dropout.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuIMBogkHTVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modify according to kaggle username and key \n",
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"amitkagglepractice\",\"key\":\"fbb6aeed8826cde8a23312712a265de1\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPpyi34NHoPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrI7wQnHr8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "10b1bc85-5acc-4c9e-b07f-f608bb5129da"
      },
      "source": [
        "# train_v2.csv \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p data\n",
        "# unzipping\n",
        "% cd /content/data\n",
        "!unzip train_v2.csv.zip -d /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading train_v2.csv.zip to data\n",
            "  0% 0.00/159k [00:00<?, ?B/s]\n",
            "100% 159k/159k [00:00<00:00, 49.1MB/s]\n",
            "/content/data\n",
            "Archive:  train_v2.csv.zip\n",
            "  inflating: /content/data/train_v2.csv  \n",
            "   creating: /content/data/__MACOSX/\n",
            "  inflating: /content/data/__MACOSX/._train_v2.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bIw5_cAHweK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "a64a426f-6d12-4e48-9daf-475e6ceaedc0"
      },
      "source": [
        "# train-jpg.tar.7z\n",
        "# this should take a little time - loading all the images \n",
        "\n",
        "# downloading from kaggle \n",
        "% cd /content\n",
        "!kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p data\n",
        "# unzipping \n",
        "% cd /content/data\n",
        "!7z x -so train-jpg.tar.7z | tar xf - -C /content/data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading train-jpg.tar.7z to data\n",
            " 99% 597M/600M [00:06<00:00, 143MB/s]\n",
            "100% 600M/600M [00:06<00:00, 97.1MB/s]\n",
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "repVlwV2Ixnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "81b093b4-dc34-4e5b-9a61-4821434d8f5e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_label = pd.read_csv('/content/data/train_v2.csv')\n",
        "train_label.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>haze primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>agriculture clear primary water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>clear primary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>agriculture clear habitation primary road</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_name                                       tags\n",
              "0    train_0                               haze primary\n",
              "1    train_1            agriculture clear primary water\n",
              "2    train_2                              clear primary\n",
              "3    train_3                              clear primary\n",
              "4    train_4  agriculture clear habitation primary road"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNJLjodgJFv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = '/content/data/train-jpg/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aei5tlfpJNdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "5264a120-bd6c-41a8-8f19-e25bb3e477d1"
      },
      "source": [
        "mapping_csv = train_label\n",
        "# summarize properties\n",
        "print(mapping_csv.shape)\n",
        "print(mapping_csv[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40479, 2)\n",
            "  image_name                                         tags\n",
            "0    train_0                                 haze primary\n",
            "1    train_1              agriculture clear primary water\n",
            "2    train_2                                clear primary\n",
            "3    train_3                                clear primary\n",
            "4    train_4    agriculture clear habitation primary road\n",
            "5    train_5                           haze primary water\n",
            "6    train_6  agriculture clear cultivation primary water\n",
            "7    train_7                                 haze primary\n",
            "8    train_8        agriculture clear cultivation primary\n",
            "9    train_9   agriculture clear cultivation primary road\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJcAynXZJTua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a set of labels\n",
        "labels = set()\n",
        "for i in range(len(mapping_csv)):\n",
        "  # convert spaced separated tags into an array of tags\n",
        "  tags = mapping_csv['tags'][i].split(' ')\n",
        "  # add tags to the set of known labels\n",
        "  labels.update(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqRBT5fAJYby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert set of labels to a list to list\n",
        "labels = list(labels)\n",
        "# order set alphabetically\n",
        "labels.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ent7LSHYJbkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict that maps labels to integers, and the reverse\n",
        "labels_map = {labels[i]:i for i in range(len(labels))}\n",
        "inv_labels_map = {i:labels[i] for i in range(len(labels))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTNVPhoYJfnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping of tags to integers given the loaded mapping file\n",
        "def create_tag_mapping(mapping_csv):\n",
        "\t# create a set of all known tags\n",
        "\tlabels = set()\n",
        "\tfor i in range(len(mapping_csv)):\n",
        "\t\t# convert spaced separated tags into an array of tags\n",
        "\t\ttags = mapping_csv['tags'][i].split(' ')\n",
        "\t\t# add tags to the set of known labels\n",
        "\t\tlabels.update(tags)\n",
        "\t# convert set of labels to a list to list\n",
        "\tlabels = list(labels)\n",
        "\t# order set alphabetically\n",
        "\tlabels.sort()\n",
        "\t# dict that maps labels to integers, and the reverse\n",
        "\tlabels_map = {labels[i]:i for i in range(len(labels))}\n",
        "\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "\treturn labels_map, inv_labels_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkojomdNJk3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7978180e-84d6-4b1f-f9d5-bee5dacf2ff0"
      },
      "source": [
        "mapping, inv_mapping = create_tag_mapping(mapping_csv)\n",
        "print(len(mapping))\n",
        "print(mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSAOYqNFJs9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping of filename to tags\n",
        "def create_file_mapping(mapping_csv):\n",
        "  mapping = dict()\n",
        "  for i in range(len(mapping_csv)):\n",
        "    name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
        "    mapping[name] = tags.split(' ')\n",
        "  return mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq-JMGTCJxBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "\t# create empty vector\n",
        "\tencoding = zeros(len(mapping), dtype='uint8')\n",
        "\t# mark 1 for each tag in the vector\n",
        "\tfor tag in tags:\n",
        "\t\tencoding[mapping[tag]] = 1\n",
        "\treturn encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RymkcHtJ09j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3d07bb0-c94f-4f2a-a34e-bba8ee2ba4d2"
      },
      "source": [
        "# load and prepare planet dataset and save to file\n",
        "from os import listdir\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from pandas import read_csv\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umFOXRUiKCve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all images into memory\n",
        "def load_dataset(path, file_mapping, tag_mapping):\n",
        "\tphotos, targets = list(), list()\n",
        "\t# enumerate files in the directory\n",
        "\tfor filename in listdir(folder):\n",
        "\t\t# load image\n",
        "\t\tphoto = load_img(path + filename, target_size=(128,128))\n",
        "\t\t# convert to numpy array\n",
        "\t\tphoto = img_to_array(photo, dtype='uint8')\n",
        "\t\t# get tags\n",
        "\t\ttags = file_mapping[filename[:-4]]\n",
        "\t\t# one hot encode tags\n",
        "\t\ttarget = one_hot_encode(tags, tag_mapping)\n",
        "\t\t# store\n",
        "\t\tphotos.append(photo)\n",
        "\t\ttargets.append(target)\n",
        "\tX = asarray(photos, dtype='uint8')\n",
        "\ty = asarray(targets, dtype='uint8')\n",
        "\treturn X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMHjTlUKKHkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the mapping file\n",
        "filename = '/content/data/train_v2.csv'\n",
        "mapping_csv = read_csv(filename)\n",
        "# create a mapping of tags to integers\n",
        "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
        "# create a mapping of filenames to tag lists\n",
        "file_mapping = create_file_mapping(mapping_csv)\n",
        "# load the jpeg images\n",
        "folder = '/content/data/train-jpg/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJHKSQSTKMZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9feed5c5-bf28-4dd3-a9a0-313e00724958"
      },
      "source": [
        "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40479, 128, 128, 3) (40479, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-oSGhX9Kmi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save both arrays to one file in compressed format\n",
        "savez_compressed('planet_data.npz', X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzUJD021NJT_",
        "colab_type": "text"
      },
      "source": [
        "## Dropout Regularization\n",
        "\n",
        "Dropout regularization is a computationally cheap way to regularize a deep neural network.\n",
        "\n",
        "Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks with very different network structure and, in turn, making nodes in the network generally more robust to the inputs.\n",
        "\n",
        "Typically, a small amount of dropout can be applied after each VGG block, with more dropout applied to the fully connected layers near the output layer of the model.\n",
        "\n",
        "Below is the define_model() function for an updated version of the baseline model with the addition of Dropout. In this case, a dropout of 20% is applied after each VGG block, with a larger dropout rate of 50% applied after the fully connected layer in the classifier part of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9XVrQCaLJwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6d07d098-20eb-4962-90a0-43e971d47979"
      },
      "source": [
        "# baseline model with dropout on the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        " \n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('planet_data.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        " \n",
        "# define cnn model\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(out_shape, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0)\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        " \n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 18:29:45.934479 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 18:29:45.969964 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 18:29:45.977491 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 18:29:46.016749 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0724 18:29:46.019083 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0724 18:29:46.028376 139654068885376 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0724 18:29:46.177290 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0724 18:29:46.183128 139654068885376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0724 18:29:46.187980 139654068885376 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGSbCxEgNrKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}